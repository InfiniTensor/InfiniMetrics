#!/usr/bin/env python3
"""Inference tests analysis page."""

import streamlit as st
import pandas as pd
from pathlib import Path
import sys

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from components.header import render_header
from utils.data_loader import InfiniMetricsDataLoader, get_friendly_size
from utils.visualizations import (
    plot_timeseries_auto,
    create_summary_table_infer,
)

st.set_page_config(page_title="æ¨ç†æµ‹è¯•åˆ†æ | InfiniMetrics", page_icon="ğŸ¤–", layout="wide")

if "data_loader" not in st.session_state:
    st.session_state.data_loader = InfiniMetricsDataLoader()

# Sync MongoDB setting from main page
if "use_mongodb" not in st.session_state:
    st.session_state.use_mongodb = False


def main():
    render_header()
    st.markdown("## ğŸš€ æ¨ç†æ€§èƒ½æµ‹è¯•åˆ†æ")

    dl = st.session_state.data_loader

    # Show current data source
    if dl.source_type == "mongodb":
        st.caption("ğŸŸ¢ æ•°æ®æº: MongoDB")
    else:
        st.caption("ğŸ“ æ•°æ®æº: æ–‡ä»¶ç³»ç»Ÿ")

    runs = dl.list_test_runs("infer")

    if not runs:
        st.info("æœªæ‰¾åˆ°æ¨ç†æµ‹è¯•ç»“æœï¼ˆtestcase éœ€ä»¥ infer.* å¼€å¤´ï¼‰ã€‚")
        return

    # ---------- Sidebar Filters ----------
    with st.sidebar:
        st.markdown("### ğŸ” ç­›é€‰æ¡ä»¶")

        frameworks = sorted(
            set((r.get("config", {}).get("framework") or "unknown") for r in runs)
        )
        selected_fw = st.multiselect("æ¡†æ¶", frameworks, default=frameworks)

        modes = []
        for r in runs:
            tc = r.get("testcase", "")
            if "Service" in tc or "service" in tc.lower():
                modes.append("service")
            else:
                modes.append("direct")
        modes = sorted(set(modes))
        selected_modes = st.multiselect("æ¨¡å¼", modes, default=modes)

        device_counts = sorted(set(r.get("device_used", 1) for r in runs))
        selected_dev = st.multiselect("è®¾å¤‡æ•°", device_counts, default=device_counts)

        only_success = st.checkbox("ä»…æ˜¾ç¤ºæˆåŠŸæµ‹è¯•", value=True)

        y_log = st.checkbox("Yè½´å¯¹æ•°åˆ»åº¦ï¼ˆéƒ¨åˆ†æ›²çº¿æ›´æ¸…æ™°ï¼‰", value=False)

    def _mode_of(r):
        tc = r.get("testcase") or ""
        return "service" if ("Service" in tc or "service" in tc.lower()) else "direct"

    filtered = [
        r
        for r in runs
        if (
            not selected_fw
            or (r.get("config", {}).get("framework") or "unknown") in selected_fw
        )
        and (not selected_modes or _mode_of(r) in selected_modes)
        and (not selected_dev or r.get("device_used", 1) in selected_dev)
        and (not only_success or r.get("success"))
    ]

    st.caption(f"æ‰¾åˆ° {len(filtered)} ä¸ªæ¨ç†æµ‹è¯•")

    if not filtered:
        st.warning("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æµ‹è¯•ç»“æœ")
        return

    # ---------- Run Selection ----------
    options = {
        f"{r.get('config', {}).get('framework','unknown')} | {_mode_of(r)} | {r.get('device_used','?')} GPU | {r.get('time','')} | {r.get('run_id','')[:10]}": i
        for i, r in enumerate(filtered)
    }

    selected = st.multiselect(
        "é€‰æ‹©è¦åˆ†æçš„æµ‹è¯•è¿è¡Œï¼ˆå¯å¤šé€‰å¯¹æ¯”ï¼‰",
        list(options.keys()),
        default=list(options.keys())[:1],
    )
    if not selected:
        return

    selected_runs = []
    for k in selected:
        ri = filtered[options[k]]
        # Use run_id for MongoDB, path for file system
        identifier = ri.get("run_id") if dl.source_type == "mongodb" else ri.get("path")
        data = dl.load_test_result(identifier)
        ri = dict(ri)
        ri["data"] = data
        selected_runs.append(ri)

    tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ æ€§èƒ½å›¾è¡¨", "ğŸ“Š æ•°æ®è¡¨æ ¼", "ğŸ” è¯¦ç»†é…ç½®"])

    # ---------- Charts ----------
    with tab1:
        st.markdown("### æŒ‡æ ‡æ›²çº¿ï¼ˆLatency / TTFT / Throughputï¼‰")

        # layout: 3 columns
        c1, c2, c3 = st.columns(3)

        def _plot_metric(metric_name_contains: str, container):
            with container:
                if len(selected_runs) == 1:
                    run = selected_runs[0]
                    metrics = run["data"].get("metrics", [])
                    hit = next(
                        (
                            m
                            for m in metrics
                            if metric_name_contains in (m.get("name", ""))
                            and m.get("data") is not None
                        ),
                        None,
                    )
                    if hit:
                        df = hit["data"]
                        fig = plot_timeseries_auto(
                            df,
                            title=f"{hit['name']} - {run.get('config',{}).get('framework','')}",
                            y_log_scale=y_log,
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info(f"æœªæ‰¾åˆ° {metric_name_contains} å¯¹åº”çš„ CSV")
                else:
                    # multi-run comparison: overlay lines
                    st.markdown(f"**å¯¹æ¯”ï¼š{metric_name_contains}**")
                    lines = []
                    for run in selected_runs:
                        hit = next(
                            (
                                m
                                for m in run["data"].get("metrics", [])
                                if metric_name_contains in (m.get("name", ""))
                                and m.get("data") is not None
                            ),
                            None,
                        )
                        if not hit:
                            continue
                        lines.append((run, hit))
                    if not lines:
                        st.info("é€‰ä¸­çš„è¿è¡Œä¸­æ²¡æœ‰å¯ç”¨æ•°æ®")
                        return

                    import plotly.graph_objects as go

                    fig = go.Figure()
                    for run, hit in lines:
                        df = hit["data"]
                        xcol = df.columns[0]
                        ycol = df.columns[1] if len(df.columns) > 1 else None
                        if ycol is None:
                            continue
                        label = f"{run.get('config',{}).get('framework','unknown')}|{_mode_of(run)}|{run.get('device_used','?')}GPU"
                        fig.add_trace(
                            go.Scatter(
                                x=df[xcol], y=df[ycol], mode="lines+markers", name=label
                            )
                        )
                    fig.update_layout(
                        title=f"{metric_name_contains} å¯¹æ¯”",
                        xaxis_title="step",
                        yaxis_title=metric_name_contains,
                        template="plotly_white",
                        height=420,
                    )
                    if y_log:
                        fig.update_yaxes(type="log")
                    st.plotly_chart(fig, use_container_width=True)

        _plot_metric("infer.compute_latency", c1)
        _plot_metric("infer.ttft", c2)
        _plot_metric("infer.direct_throughput", c3)

    # ---------- Tables ----------
    with tab2:
        for run in selected_runs:
            with st.expander(f"{run.get('run_id')} - åŸå§‹æ•°æ®"):
                for m in run["data"].get("metrics", []):
                    if m.get("data") is None:
                        continue
                    st.markdown(f"**{m.get('name')}**")
                    st.dataframe(m["data"], use_container_width=True, hide_index=True)

    # ---------- Config ----------
    with tab3:
        for run in selected_runs:
            with st.expander(f"{run.get('run_id')} - é…ç½®ä¸ç¯å¢ƒ"):
                summary = create_summary_table_infer(run["data"])
                st.dataframe(summary, use_container_width=True, hide_index=True)
                st.markdown("**config**")
                st.json(run["data"].get("config", {}))
                st.markdown("**resolved**")
                st.json(run["data"].get("resolved", {}))
                st.markdown("**environment**")
                st.json(run["data"].get("environment", {}))


if __name__ == "__main__":
    main()
